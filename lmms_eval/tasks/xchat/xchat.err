/home/seungonk/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.41s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.13s/it]
  0%|          | 0/50 [00:00<?, ?it/s]Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.
Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  2%|▏         | 1/50 [00:38<31:06, 38.09s/it]  4%|▍         | 2/50 [00:44<15:28, 19.34s/it]  6%|▌         | 3/50 [00:48<09:48, 12.52s/it]  8%|▊         | 4/50 [01:11<12:38, 16.49s/it] 10%|█         | 5/50 [01:15<09:03, 12.07s/it] 12%|█▏        | 6/50 [01:21<07:21, 10.03s/it] 14%|█▍        | 7/50 [01:35<08:04, 11.27s/it] 16%|█▌        | 8/50 [01:57<10:22, 14.82s/it] 18%|█▊        | 9/50 [02:02<08:02, 11.76s/it] 20%|██        | 10/50 [02:04<05:50,  8.75s/it] 22%|██▏       | 11/50 [02:07<04:30,  6.95s/it] 24%|██▍       | 12/50 [02:20<05:36,  8.85s/it] 26%|██▌       | 13/50 [02:25<04:43,  7.66s/it] 28%|██▊       | 14/50 [02:30<03:58,  6.63s/it] 30%|███       | 15/50 [02:39<04:25,  7.60s/it] 32%|███▏      | 16/50 [02:58<06:05, 10.76s/it] 34%|███▍      | 17/50 [03:22<08:14, 14.98s/it] 36%|███▌      | 18/50 [03:25<06:04, 11.39s/it] 38%|███▊      | 19/50 [03:50<07:57, 15.41s/it] 40%|████      | 20/50 [04:04<07:27, 14.92s/it] 42%|████▏     | 21/50 [04:41<10:21, 21.43s/it] 44%|████▍     | 22/50 [04:53<08:41, 18.64s/it] 46%|████▌     | 23/50 [05:10<08:14, 18.33s/it] 48%|████▊     | 24/50 [05:22<07:05, 16.38s/it] 50%|█████     | 25/50 [06:02<09:45, 23.43s/it] 52%|█████▏    | 26/50 [06:15<08:06, 20.29s/it] 54%|█████▍    | 27/50 [06:35<07:46, 20.30s/it] 56%|█████▌    | 28/50 [06:44<06:09, 16.78s/it] 58%|█████▊    | 29/50 [06:55<05:18, 15.18s/it] 60%|██████    | 30/50 [07:18<05:48, 17.43s/it] 62%|██████▏   | 31/50 [07:25<04:32, 14.36s/it] 64%|██████▍   | 32/50 [07:35<03:56, 13.11s/it] 66%|██████▌   | 33/50 [07:59<04:38, 16.40s/it] 68%|██████▊   | 34/50 [08:04<03:27, 12.99s/it] 70%|███████   | 35/50 [08:17<03:13, 12.90s/it] 72%|███████▏  | 36/50 [08:34<03:16, 14.04s/it] 74%|███████▍  | 37/50 [08:50<03:09, 14.57s/it] 76%|███████▌  | 38/50 [08:56<02:26, 12.20s/it] 78%|███████▊  | 39/50 [09:01<01:48,  9.84s/it] 80%|████████  | 40/50 [09:08<01:29,  8.97s/it] 82%|████████▏ | 41/50 [09:11<01:05,  7.29s/it] 84%|████████▍ | 42/50 [09:25<01:14,  9.33s/it] 86%|████████▌ | 43/50 [09:31<00:58,  8.41s/it] 88%|████████▊ | 44/50 [10:05<01:35, 15.94s/it] 90%|█████████ | 45/50 [10:20<01:18, 15.76s/it] 92%|█████████▏| 46/50 [10:24<00:48, 12.07s/it] 94%|█████████▍| 47/50 [10:26<00:27,  9.11s/it] 96%|█████████▌| 48/50 [10:29<00:14,  7.23s/it] 98%|█████████▊| 49/50 [10:32<00:06,  6.04s/it]100%|██████████| 50/50 [10:34<00:00,  4.85s/it]100%|██████████| 50/50 [10:34<00:00, 12.69s/it]
/home/seungonk/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.65s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.23s/it]
  0%|          | 0/50 [00:00<?, ?it/s]Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.
Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  2%|▏         | 1/50 [01:13<59:43, 73.13s/it]  4%|▍         | 2/50 [01:49<41:25, 51.79s/it]  6%|▌         | 3/50 [02:39<39:38, 50.61s/it]  8%|▊         | 4/50 [03:20<35:59, 46.94s/it] 10%|█         | 5/50 [04:01<33:33, 44.74s/it] 12%|█▏        | 6/50 [05:04<37:23, 50.99s/it] 14%|█▍        | 7/50 [05:29<30:31, 42.59s/it] 16%|█▌        | 8/50 [05:50<25:00, 35.72s/it] 18%|█▊        | 9/50 [06:43<27:57, 40.91s/it] 20%|██        | 10/50 [07:02<22:51, 34.28s/it] 22%|██▏       | 11/50 [07:16<18:19, 28.20s/it] 24%|██▍       | 12/50 [08:27<26:01, 41.09s/it] 26%|██▌       | 13/50 [09:39<31:01, 50.31s/it] 28%|██▊       | 14/50 [10:38<31:54, 53.19s/it] 30%|███       | 15/50 [11:06<26:29, 45.43s/it] 32%|███▏      | 16/50 [12:18<30:14, 53.37s/it] 34%|███▍      | 17/50 [13:29<32:23, 58.88s/it] 36%|███▌      | 18/50 [13:47<24:51, 46.62s/it] 38%|███▊      | 19/50 [14:25<22:39, 43.86s/it] 40%|████      | 20/50 [15:26<24:32, 49.09s/it] 42%|████▏     | 21/50 [16:38<27:01, 55.93s/it] 44%|████▍     | 22/50 [17:10<22:42, 48.65s/it] 46%|████▌     | 23/50 [18:21<24:57, 55.47s/it] 48%|████▊     | 24/50 [19:31<25:55, 59.84s/it] 50%|█████     | 25/50 [20:14<22:51, 54.87s/it] 52%|█████▏    | 26/50 [21:28<24:10, 60.46s/it] 54%|█████▍    | 27/50 [22:39<24:26, 63.75s/it] 56%|█████▌    | 28/50 [23:51<24:16, 66.21s/it] 58%|█████▊    | 29/50 [24:21<19:19, 55.21s/it] 60%|██████    | 30/50 [25:32<20:01, 60.06s/it] 62%|██████▏   | 31/50 [26:22<18:00, 56.86s/it] 64%|██████▍   | 32/50 [26:59<15:16, 50.89s/it] 66%|██████▌   | 33/50 [27:19<11:52, 41.90s/it] 68%|██████▊   | 34/50 [28:14<12:09, 45.60s/it] 70%|███████   | 35/50 [29:02<11:37, 46.53s/it] 72%|███████▏  | 36/50 [30:12<12:28, 53.44s/it] 74%|███████▍  | 37/50 [31:26<12:54, 59.56s/it] 76%|███████▌  | 38/50 [32:38<12:38, 63.22s/it] 78%|███████▊  | 39/50 [33:49<12:04, 65.83s/it] 80%|████████  | 40/50 [34:43<10:20, 62.07s/it] 82%|████████▏ | 41/50 [35:54<09:44, 64.97s/it] 84%|████████▍ | 42/50 [36:37<07:45, 58.25s/it] 86%|████████▌ | 43/50 [37:39<06:54, 59.26s/it] 88%|████████▊ | 44/50 [38:52<06:21, 63.52s/it] 90%|█████████ | 45/50 [39:50<05:08, 61.67s/it] 92%|█████████▏| 46/50 [40:05<03:11, 47.97s/it] 94%|█████████▍| 47/50 [40:13<01:47, 35.87s/it] 96%|█████████▌| 48/50 [40:47<01:10, 35.29s/it] 98%|█████████▊| 49/50 [41:19<00:34, 34.32s/it]100%|██████████| 50/50 [41:39<00:00, 30.02s/it]100%|██████████| 50/50 [41:39<00:00, 49.99s/it]
/home/seungonk/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.12it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  9.75it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  9.20it/s]
  0%|          | 0/50 [00:00<?, ?it/s]Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.
Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  2%|▏         | 1/50 [00:08<06:37,  8.12s/it]  4%|▍         | 2/50 [00:11<04:12,  5.27s/it]  6%|▌         | 3/50 [00:20<05:23,  6.88s/it]  8%|▊         | 4/50 [00:24<04:35,  5.99s/it] 10%|█         | 5/50 [00:28<03:44,  5.00s/it] 12%|█▏        | 6/50 [00:41<05:50,  7.96s/it] 14%|█▍        | 7/50 [00:53<06:28,  9.04s/it] 16%|█▌        | 8/50 [01:02<06:23,  9.13s/it] 18%|█▊        | 9/50 [01:11<06:13,  9.10s/it] 20%|██        | 10/50 [01:30<08:03, 12.09s/it] 22%|██▏       | 11/50 [01:39<07:23, 11.37s/it] 24%|██▍       | 12/50 [01:44<05:47,  9.16s/it] 26%|██▌       | 13/50 [01:55<06:06,  9.89s/it] 28%|██▊       | 14/50 [02:00<05:00,  8.35s/it] 30%|███       | 15/50 [02:07<04:37,  7.93s/it] 32%|███▏      | 16/50 [02:13<04:14,  7.48s/it] 34%|███▍      | 17/50 [02:19<03:45,  6.85s/it] 36%|███▌      | 18/50 [02:28<04:01,  7.55s/it] 38%|███▊      | 19/50 [02:36<04:03,  7.87s/it] 40%|████      | 20/50 [02:46<04:11,  8.40s/it] 42%|████▏     | 21/50 [02:57<04:26,  9.20s/it] 44%|████▍     | 22/50 [03:03<03:49,  8.18s/it] 46%|████▌     | 23/50 [03:11<03:37,  8.05s/it] 48%|████▊     | 24/50 [03:18<03:20,  7.69s/it] 50%|█████     | 25/50 [03:24<03:05,  7.43s/it] 52%|█████▏    | 26/50 [03:31<02:55,  7.32s/it] 54%|█████▍    | 27/50 [03:38<02:41,  7.01s/it] 56%|█████▌    | 28/50 [03:48<02:55,  7.98s/it] 58%|█████▊    | 29/50 [03:56<02:45,  7.89s/it] 60%|██████    | 30/50 [04:02<02:30,  7.52s/it] 62%|██████▏   | 31/50 [04:16<02:56,  9.32s/it] 64%|██████▍   | 32/50 [04:25<02:48,  9.38s/it] 66%|██████▌   | 33/50 [04:37<02:50, 10.05s/it] 68%|██████▊   | 34/50 [04:51<03:02, 11.40s/it] 70%|███████   | 35/50 [05:01<02:42, 10.81s/it] 72%|███████▏  | 36/50 [05:11<02:28, 10.62s/it] 74%|███████▍  | 37/50 [05:17<02:00,  9.24s/it] 76%|███████▌  | 38/50 [05:23<01:39,  8.25s/it] 78%|███████▊  | 39/50 [05:29<01:23,  7.57s/it] 80%|████████  | 40/50 [05:39<01:22,  8.21s/it] 82%|████████▏ | 41/50 [05:42<01:01,  6.81s/it] 84%|████████▍ | 42/50 [05:46<00:45,  5.73s/it] 86%|████████▌ | 43/50 [05:49<00:36,  5.15s/it] 88%|████████▊ | 44/50 [05:55<00:31,  5.25s/it] 90%|█████████ | 45/50 [05:57<00:21,  4.36s/it] 92%|█████████▏| 46/50 [06:04<00:20,  5.03s/it] 94%|█████████▍| 47/50 [06:23<00:28,  9.42s/it] 96%|█████████▌| 48/50 [06:30<00:17,  8.55s/it] 98%|█████████▊| 49/50 [06:36<00:07,  7.70s/it]100%|██████████| 50/50 [06:45<00:00,  8.28s/it]100%|██████████| 50/50 [06:45<00:00,  8.11s/it]
/home/seungonk/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.07it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  9.69it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  9.14it/s]
  0%|          | 0/50 [00:00<?, ?it/s]Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.
Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  2%|▏         | 1/50 [00:04<03:50,  4.70s/it]  4%|▍         | 2/50 [00:14<06:01,  7.53s/it]  6%|▌         | 3/50 [00:37<11:32, 14.73s/it]  8%|▊         | 4/50 [01:07<15:49, 20.65s/it] 10%|█         | 5/50 [01:22<13:57, 18.61s/it] 12%|█▏        | 6/50 [02:08<20:39, 28.16s/it] 14%|█▍        | 7/50 [02:44<21:57, 30.64s/it] 16%|█▌        | 8/50 [03:18<22:04, 31.53s/it] 18%|█▊        | 9/50 [04:20<28:06, 41.13s/it] 20%|██        | 10/50 [04:55<26:12, 39.32s/it] 22%|██▏       | 11/50 [05:26<23:52, 36.72s/it] 24%|██▍       | 12/50 [06:00<22:49, 36.04s/it] 26%|██▌       | 13/50 [06:31<21:14, 34.45s/it] 28%|██▊       | 14/50 [06:54<18:31, 30.86s/it] 30%|███       | 15/50 [07:13<15:57, 27.35s/it] 32%|███▏      | 16/50 [07:24<12:42, 22.44s/it] 34%|███▍      | 17/50 [07:52<13:12, 24.01s/it] 36%|███▌      | 18/50 [08:19<13:17, 24.93s/it] 38%|███▊      | 19/50 [08:40<12:15, 23.71s/it] 40%|████      | 20/50 [09:20<14:23, 28.80s/it] 42%|████▏     | 21/50 [09:24<10:19, 21.35s/it] 44%|████▍     | 22/50 [10:02<12:14, 26.24s/it] 46%|████▌     | 23/50 [10:26<11:33, 25.69s/it] 48%|████▊     | 24/50 [10:53<11:16, 26.03s/it] 50%|█████     | 25/50 [11:41<13:35, 32.62s/it] 52%|█████▏    | 26/50 [11:52<10:25, 26.06s/it] 54%|█████▍    | 27/50 [12:12<09:18, 24.28s/it] 56%|█████▌    | 28/50 [12:36<08:51, 24.16s/it] 58%|█████▊    | 29/50 [13:07<09:09, 26.17s/it] 60%|██████    | 30/50 [13:35<08:53, 26.68s/it] 62%|██████▏   | 31/50 [14:22<10:24, 32.87s/it] 64%|██████▍   | 32/50 [14:41<08:35, 28.64s/it] 66%|██████▌   | 33/50 [15:19<08:55, 31.50s/it] 68%|██████▊   | 34/50 [16:05<09:33, 35.87s/it] 70%|███████   | 35/50 [16:31<08:14, 32.98s/it] 72%|███████▏  | 36/50 [17:21<08:50, 37.92s/it] 74%|███████▍  | 37/50 [17:50<07:38, 35.29s/it] 76%|███████▌  | 38/50 [18:40<07:56, 39.70s/it] 78%|███████▊  | 39/50 [19:22<07:24, 40.43s/it] 80%|████████  | 40/50 [19:51<06:09, 36.97s/it] 82%|████████▏ | 41/50 [20:01<04:19, 28.85s/it] 84%|████████▍ | 42/50 [20:04<02:49, 21.19s/it] 86%|████████▌ | 43/50 [20:25<02:27, 21.09s/it] 88%|████████▊ | 44/50 [20:31<01:39, 16.59s/it] 90%|█████████ | 45/50 [20:47<01:21, 16.37s/it] 92%|█████████▏| 46/50 [21:25<01:31, 22.90s/it] 94%|█████████▍| 47/50 [21:39<01:00, 20.25s/it] 96%|█████████▌| 48/50 [22:10<00:46, 23.33s/it] 98%|█████████▊| 49/50 [22:42<00:26, 26.10s/it]100%|██████████| 50/50 [23:23<00:00, 30.40s/it]100%|██████████| 50/50 [23:23<00:00, 28.06s/it]
